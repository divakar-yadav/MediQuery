{
    "title": "DACBT: deep learning approach for classification of brain tumors using MRI data in IoT healthcare environment",
    "abstract": "The high performance of the proposed method demonstrated that it is correctly classified brain tumors (meningioma, glioma, and pictutitary), and it can easily be deployed in IoT-health care for the classification of brain tumors.Table 6Comparison of ResNet-CNN model accuracy with previous models.ModelAcc (%)RefSpace complexityTime cmplexityANN and KNN97, 9810\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((nm + mk) + (n*d) )$$\\end{document}O((nm+mk)+(n\u2217d))\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((cwh) + (nd+Kn))$$\\end{document}O((cwh)+(nd+Kn))GA-CNN94.212\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)SVM and KNN85, 888\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((n) + (n*d))$$\\end{document}O((n)+(n\u2217d))\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((n^2) + (nd+Kn))$$\\end{document}O((n2)+(nd+Kn))CNN-TF94.8213\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)Proposed method ResNet-CNN99.902022\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$c=$$\\end{document}c= the number of convolutional channels, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$h=$$\\end{document}h=\u00a0height of input, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$w=$$\\end{document}w= width of input, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$f=$$\\end{document}f= the convolutional kernel size, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$n=$$\\end{document}n= the number data instances, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$k=$$\\end{document}k= the number of output neurons, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$m=$$\\end{document}m= the number of input neurons and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$d =$$\\end{document}d= the dimension or feature of the input, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$K=$$\\end{document}K=\u00a0number of nearest neighbors, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$u=c*w*h$$\\end{document}u=c\u2217w\u2217h.Figure 5ResNet-CNN model performance comparison with baseline models show that our model predictive performance in terms of accuracy is high from baseline models. The high performance of the proposed method demonstrated that it is correctly classified brain tumors (meningioma, glioma, and pictutitary), and it can easily be deployed in IoT-health care for the classification of brain tumors.Table 6Comparison of ResNet-CNN model accuracy with previous models.ModelAcc (%)RefSpace complexityTime cmplexityANN and KNN97, 9810\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((nm + mk) + (n*d) )$$\\end{document}O((nm+mk)+(n\u2217d))\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((cwh) + (nd+Kn))$$\\end{document}O((cwh)+(nd+Kn))GA-CNN94.212\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)SVM and KNN85, 888\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((n) + (n*d))$$\\end{document}O((n)+(n\u2217d))\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((n^2) + (nd+Kn))$$\\end{document}O((n2)+(nd+Kn))CNN-TF94.8213\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)Proposed method ResNet-CNN99.902022\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$c=$$\\end{document}c= the number of convolutional channels, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$h=$$\\end{document}h=\u00a0height of input, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$w=$$\\end{document}w= width of input, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$f=$$\\end{document}f= the convolutional kernel size, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$n=$$\\end{document}n= the number data instances, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$k=$$\\end{document}k= the number of output neurons, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$m=$$\\end{document}m= the number of input neurons and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$d =$$\\end{document}d= the dimension or feature of the input, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$K=$$\\end{document}K=\u00a0number of nearest neighbors, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$u=c*w*h$$\\end{document}u=c\u2217w\u2217h.Figure 5ResNet-CNN model performance comparison with baseline models show that our model predictive performance in terms of accuracy is high from baseline models. The high performance of the proposed method demonstrated that it is correctly classified brain tumors (meningioma, glioma, and pictutitary), and it can easily be deployed in IoT-health care for the classification of brain tumors.Table 6Comparison of ResNet-CNN model accuracy with previous models.ModelAcc (%)RefSpace complexityTime cmplexityANN and KNN97, 9810\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((nm + mk) + (n*d) )$$\\end{document}O((nm+mk)+(n\u2217d))\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((cwh) + (nd+Kn))$$\\end{document}O((cwh)+(nd+Kn))GA-CNN94.212\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)SVM and KNN85, 888\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((n) + (n*d))$$\\end{document}O((n)+(n\u2217d))\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}((n^2) + (nd+Kn))$$\\end{document}O((n2)+(nd+Kn))CNN-TF94.8213\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)Proposed method ResNet-CNN99.902022\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$c=$$\\end{document}c= the number of convolutional channels, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$h=$$\\end{document}h=\u00a0height of input, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$w=$$\\end{document}w= width of input, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$f=$$\\end{document}f= the convolutional kernel size, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$n=$$\\end{document}n= the number data instances, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$k=$$\\end{document}k= the number of output neurons, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$m=$$\\end{document}m= the number of input neurons and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$d =$$\\end{document}d= the dimension or feature of the input, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$K=$$\\end{document}K=\u00a0number of nearest neighbors, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$u=c*w*h$$\\end{document}u=c\u2217w\u2217h.Figure 5ResNet-CNN model performance comparison with baseline models show that our model predictive performance in terms of accuracy is high from baseline models.",
    "authors": [
        "Amin ul Haq",
        "Jian Ping Li",
        "Shakir Khan",
        "Mohammed Ali Alshara",
        "Reemiah Muneer Alotaibi",
        "CobbinahBernard Mawuli"
    ],
    "published_year": "2022",
    "description": "Introduction\n      Brain tumor (BT) is a series medical problem, and many people are suffering from it globally1. Because of its critical nature, brain tumours are one of the most dangerous types of brain cancer. Compared to other cancers from brain cancer less number of people are suffering2. Meningioma, Glioma, Pituitary, and Acoustic Neuroma are examples of brain tumors. In medical observation, the rates of Meningioma, GLioma, and Pituitary tumours in all brain tumors are 15%, 45%, and 15%, respectively3. A brain tumor has long-term and psychological consequences for the patient. Brain tumors are caused by tissue abnormalities that develop within the brain or the central spine, interfering with normal brain function. There are two types of brain tumors: benign and malignant. Benign brain tumors are not cancerous and grow slowly. They do not spread and are not common. Malignant brain tumours contain cancer cells and grow rapidly in one region of the brain and spread to other parts of the brain and spine.\n      The diagnosis of brain cancer is significantly necessary for early stage for effective treatment and recovery. In this regards to classify brain tumors and identify brain cancer, different non-invasive are developed in literature by researchers and medical experts in Internet of Things (IoT) healthcare industries. In the deigning of computer automatic diagnostic systems (CADS) for brain cancer detection Machine Learning (ML) and Deep Learning (DL) models are commonly used. The diagnosis of brain cancer using images data using the DL Convolution neural network (CNN) model has grown in popularity, and the CNN model is commonly used for image classification and analysis, particularly for medical image data analysis4. The CNNs model can extract more related features from data for accurate image classification2,5,6. Furthermore, data augmentation and transfer learning techniques can also improve the predictive capability of deep learning models to effective classify the brain tumors and diagnosis brain cancer in IoT healthcare industries6,7.\n      In the literature, various methods have been proposed for brain cancer diagnosis using ML and DL learning approaches by different scholars. Zacharaki et al.8 designed a brain cancer diagnosis system to classify various grades of Glioma employing SVM and KNN machine learning model and respectively achieved 85% and 88% classification accuracy. Cheng et al.9 proposed a classification approach for brain tumor classification and augmented the tumor region for improving the classification performance. They employed three techniques for feature extraction such as Gray level co-occurrence matrix, a bag of words, and an intensity histogram. Their proposed method obtained 91.28% classification accuracy.\n      Haq et al.6 proposes an AI-based intelligent integrated framework (CNN-LSTM) for brain tumors classification and diagnosis in the IoT healthcare industry. In the integrated framework design, they have incorporated the CNN model to extract features from medical MRI data automatically. The extracted features are passed to Long short-term memory (LSTM) model to learn the dependencies in the features and finally predict the class for the tumor. Further they applied brain MRI data sets for the assessment of the proposed integrated model. Massive data is one requirement for an effective deep learning model. Since the size of our original data set is small, they utilized data augmentation approaches to increase the data set size, thereby improving the model result during training. Also used the train-test splits Cross-validation approach for hyperparameter tuning and best model selection to ensure proper model fitting. For model assessment, used well-known evaluation measures. They compared the predictive outputs of the proposed CNN-LSTM model with previous methods in the Medical Internet of Things (MIoT) healthcare industry and the model obtained high predictive performance.\n      Paul et al.4 employed axial brain tumor images for convolution neural network training. In the proposed method they used two convolution layers, two max-pooling layers, and lastly, two fully connected layers for the final classification process. The proposed approach obtained 91.43% classification accuracy. El-dahshan et al.10 designed a brain tumors classification method for 80 brain images MRI classification. They used discrete wavelet transform and PCA algorithms for reducing dimensions of data. To classify the normal and abnormal tumors, they used ANN and KNN machine learning classifiers. The classifiers ANN and KNN, achieved 97% and 98% classification accuracy respectively.\n      In another study, Afshar et al.11 proposed a brain tumor classification method employing a capsule network that combined MRI images of the brain and coarse tumor boundaries and 90.89% accuracy achieved by the proposed method. Anaraki et al.12 developed an integrated framework for brain tumor classification, and in the proposed technique, they integrated CNN and GA, and designed GA-CNN framework and obtained 94.2% accuracy. Khan et al.13 proposed brain tumors classification method employing transfer learning techniques (CNN-Transfer learning) and achieved 94.82% accuracy14. The proposed multi-classification method employing ensemble of deep features and ML algorithms and obtained high performance.\n      According to the review of the literature, current brain cancer diagnosis techniques still lack a robust predictive capability in terms of accuracy to correctly diagnose brain cancer for proper treatment and recovery. To address this issue, a novel robust method for accurately diagnosing brain cancer for proper treatment and recovery in IoT healthcare industries is required. Furthermore, the artificial intelligence based brain cancer diagnosis systems also reduce the financial costs of healthcare department.\n      In this study, we created an improved CNN model for the classification of brain MR images to diagnosis brain cancer in IoT healthcare industries. In the development of the proposed model, we used Convolution neural network model to classify brain tumors types (Meningioma, Glioma and Pituitary) employing MR images data. The CNN model is more suitable for the Meningioma, Glioma, and pituitary classification using brain tumors images data and its extract more deep features from images data for final classification. To further improve the CNN model predictive capability, we have incorporated a transfer-learning (TL) techniques for proper training of the CNN architecture, the brain MR images data is insufficient. In transfer learning, we used the well-known pre-trained models ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet. The weights generated of these pre trained models individually transferred to CNN architecture for effective training o CNN model. For the fine-tuning process, the model was trained with brain MR images data set. The generated weights of pre trained models improving CNN model final predictive performance. Additionally, the data augmentation technique is incorporated to increase the data set size for effective training of the model. We also used held-out cross-validation (CV) and performance evaluation metrics. The performance of the model compared with base lines models. The experimental results confirmed that the proposed model generated higher predictive results and it could be applied in IoT-healthcare systems easily.\n      Innovations of this study summarized as follows:In IoT healthcare systems, an improved model based on CNN and TL for classifying brain tumors using MR image data is proposed for diagnosis of brain cancer.To increase the predictive accuracy of the CNN model, TL techniques are used because the brain tumor image data is insufficient for effective training of the CNN model. Pre-trained models ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet are used to train with the well-known ImageNet data set for generating trained parameters (weights). The weights of these pre tained models are individually transfer to CNN model effective training. Fine-tuning the model CNN with brain tumor images data along with transferred weights final classification.To improve model performance, the data augmentation technique is used to increase the size of the data set for effective model training.When compared to baseline methods, our model has a high predictive performance.\n      The rest of the paper is organized as follows: In \u201cMaterials and method\u201d section data set and proposed model methodology have explored. In \u201cExperiments\u201d section the experiments are reported. In \u201cDiscussion\u201d section, we discussed the significance of the work. The conclusion and research direction of future work are reported in \u201cConclusion\u201d section.\nMaterials and method\n      \n        Data set\n        We used a brain tumor data set (BTDS) from China\u2019s Nanfang hospital and general hospital, as well as Tianjing medical university, in this study (2005 to 2010)9, and new versions in 2017 have been published. T1-Weighted Contrast-Enhanced images (TWCEI) of 233 subjects with meningioma, glioma, and pituitary tumours are included in this data set. The data set is freely accessible via the Kaggle repository15. We also used the Brain MRI Images Data Set (BMIDS) for cross dataset validation, which contains 253 MRI brain images. The tumor class in the data set has 155 images, while the non-tumor class has 98 images16.\n      \n      \n        Background of convolutional neural network (CNN) architecture\n        Deep Learning model convolutional neural networks is a kind of Feed-Forward Neural Network17. Convolutions can capture translation invariance, which means that the filter is independent of position that significantly reduces the number of parameters. The CNN model have Convolutional, Pooling, and fully connected layers. Different functions are accomplished by these layers, such as dimensionality reduction, feature extractors, and classification. During the convolution operation of the forward pass, the filter is slide on the input shape and compute the map of activation, which computing the point-wise value of each output. Further add these output to achieve the activation of that point. Designed a Sliding Filter (SF) using convolution as a linear operator, and expressed as a dot product for fast deployment. Let consider x and w are input and the kernel function, the convolution process \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(x*w)(a)$$\\end{document}(x\u2217w)(a) on time index t can be mathematically expressed in Eq. (1).1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} (x*w) a=\\int x(t) w (a-t)da \\end{aligned}$$\\end{document}(x\u2217w)a=\u222bx(t)w(a-t)da\n        In Eq. (1) a is in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{ R}^n$$\\end{document}Rn for any \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n \\ge 1$$\\end{document}n\u22651. While Parameter t is discrete. In this case, the discrete convolution can be expressed as in Eq. (2):2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} x.w (a) = \\sum _{a}x w (t-a) \\end{aligned}$$\\end{document}x.w(a)=\u2211axw(t-a)\n        However, usually use 2 or 3-dimensional convolutions in CNN model. In case of 2-dimensional image I as input and K is a two dimensional kernel and the convolution can be mathematically expressed as in Eq. (3):3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} (I*K)(i,j)=\\sum _{m}\\sum _{n}I(m,n)K(i-m,j-n) \\end{aligned}$$\\end{document}(I\u2217K)(i,j)=\u2211m\u2211nI(m,n)K(i-m,j-n)\n        If the case is 3 dimensional data image, then the convolution process can be written mathematically in Eq. (4) as follow:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} (I*K)(i,j,k)=\\sum _{m}\\sum _{n}\\sum _{l}I(m,n,l)K(i-m,j-n,k-l) \\end{aligned}$$\\end{document}(I\u2217K)(i,j,k)=\u2211m\u2211n\u2211lI(m,n,l)K(i-m,j-n,k-l)\n        In addition to gain non-linearities, two activation functions can be incorporate suc as Sigmoid and ReLU. The sigmoid activation fumction non-linearity is expressed mathematically in Eq. (5):5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\theta (x)=\\frac{1}{1+exp(-x)}, x \\in R. \\end{aligned}$$\\end{document}\u03b8(x)=11+exp(-x),x\u2208R.\n        The sigmoid non-linearity activation function is suitable when need the output to be include in the range of [0,1]. Furthermore, the sigmoid function is monotone growing which means \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lim \\limits _{n \\rightarrow +\\infty } \\theta (x)=1$$\\end{document}limn\u2192+\u221e\u03b8(x)=1, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lim \\limits _{n \\rightarrow +\\infty } \\theta (x)=0$$\\end{document}limn\u2192+\u221e\u03b8(x)=0. However, this fact may be cause vanishing gradients, when the input x is not near to 0, the neuron will be more and the gradient of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta (x)$$\\end{document}\u03b8(x) will nearly to zero and will make successive optimization difficult.\n        The second activation function is relu which is mathematically defined in Eq. (6):6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Relu(x) = max(0, x), x \\in R \\end{aligned}$$\\end{document}Relu(x)=max(0,x),x\u2208R\n        The gradient of of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$relu(x)=1$$\\end{document}relu(x)=1 for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x>0$$\\end{document}x>0 and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$relu^-(x)=0$$\\end{document}relu-(x)=0 for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x<0$$\\end{document}x<0. The relu convergence capability of is good then sigmoid non-linearities.\n        The CNN model Pooling layers are utilized to produce a statistics summary of its inputs and deduced the dimensionality without missing important information. There are different types of pooling. In the layer of Max-Pooling generate the extreme values in individually rectangular neighborhood of individual point i.e i, j, k for data of three dimensional of individual feature of input respectively, while the average values generated by the average pooling layer.\n        The last layer is fully connected with n and m respectively input and output sizes. The output layer is expressed by the parameters such as a weight matrix i.e \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W \\in M_{m, n}$$\\end{document}W\u2208Mm,n with m rows, and n columns and a bias vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$b \\in {\\textbf {R}}^m$$\\end{document}b\u2208Rm. The input vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x \\in {\\textbf {R}}^n$$\\end{document}x\u2208Rn, the fully connected output layer FC along function of activation f is expressed mathematically in Eq. (7) as:7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} FC(x): = f (Wx+b) \\in R^m \\end{aligned}$$\\end{document}FC(x):=f(Wx+b)\u2208Rm\n        In Eq. (7) Wx is the product matrix while the function f is used component wise.\n        The last layers fully connected employed for classification of problems. The CNN model architecture last layer is fully connected layers and CNN output is flattened and showed as a single vector.\n      \n      \n        Convolution neural network for brain tumors classification\n        Recently, CNN models generated significant outcomes in numerous domains, such as NLP, image classification18, and diagnosis systems. In contrast to MLPs, CNN reduces the number of neurons and parameters, which results in lower complexity and faster adaptation.\n        The CNN model has significant applications in the classification of medical images18,19. In this paper we developed the CNN networks architecture with 4 alternating convolutional layers and max-pooling layers and a dropout layer after each Conv/pooling pair. The last pooling layer connected fully layer with 256 neurons, ReLU activation function, dropout layer, and sigmoid activation function are employed for classification of brain MR images (Meningioma, Glioma, and Pituitary). In addition, we have used the optimization algorithm Stochastic Gradient Descend (SGD)20. The CNN architecture is given in Fig. 1.Figure 1CNN model architecture for classification of Brain tumors.\n      \n      \n        Improve CNN model for brain tumors classification\n        To improve CNN model predictive accuracy, we employed Data augmentation (DA) and Transfer learning (TL) techniques. The data augmentation can resolve the problem of insufficient data for model training. To expand the data amount, the zooming technique is used on original image data to produce images data with the similar label. The new created data set is used for fine tuning of the model. Th The transfer learning (TL) techniques widely used in image classification tasks21, cancer sub-type recognition22 and medical images filtering23. In this work, we used the transfer learning ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet models to enhanced the predictive performance of the proposed CNN model. The ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet pre-train models were trained on imageNet data set and transferred the trained parameters weights of these models individually to CNN model for effective training, and fine-tuned the model using the brain tumor augmented MR images data set for final classification of the CNN model.\n      \n      \n        Model cross validation and evaluation criteria\n        The holdout cross-validation6,24,25 mechanism was used for training and validation of the model. In hold out CV data is randomly assign to two sets \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_0$$\\end{document}d0 and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_1$$\\end{document}d1. The \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_0$$\\end{document}d0 and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_1$$\\end{document}d1 use for training and testing of the model respectively. In hold out CV the training data set is usually large as compare to testing data set. The is train on \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_0$$\\end{document}d0 and testing on \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_1$$\\end{document}d1. The holdout CV is suitable validation method in case when the data set is very plenty. In this study brain tumor MRI Images data set was divided into 70% for training and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$30\\%$$\\end{document}30% for teasing of the model. The performance evaluation metrics Accuracy (Acc), Sensitivity (Sn), Specificity (Sp), Precision (Pr), F1-Score (F1-S), and Matthews Correlation Coefficient (MCC)26\u201329 are used for model evaluation.\n      \n      \n        Proposed brain tumors classification model\n        NCNN models are now popular for image classification problems. A large image data set is more suitable for the CNN model\u2019s effective training, as it allows the model to extract more related features during the training process for accurate image classification. The CNN model\u2019s performance suffers as a result of the scarcity of large image data sets, particularly in the medical domain. However, to enhance the proposed CNN classifier performance, data augmentation and transfer learning6,21,30,31 techniques are incorporated. We have used transfer learning pre-trained models ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet along with data augmentation technique zooming. The imagesNet data set has been employed for pre-trained of ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet models, and the generated weights (trained parameters) of these models were transferred for the effective training of the CNN model individually. Brain tumor MRI data set was used for fine-tuning of CNN model and for final classification of the model in IoT healthcare system.\n        Furthermore, the proposed CNN model was trained and tested on a data set of brain tumour MR images, and its performance was compared to that of the transfer learning technique. A heldout cross-validation mechanism is used in the proposed method for model training and testing, with 70% used for training and 30% for model validation. The data augmentation20 technique was used to augment the original dataset by using the zooming method, which improves the model generalisation capability. The integration of data augmentation and transfer learning greatly enhanced the predictive accuracy of the CNN model. The evaluation criteria of the model different assessment metrics have used.\n        The data set X(i,\u00a0i) embedded into the CNN classifier,We used data transformations to increase the size of the data set so that we could train the model. Furthermore, the number of epochs E, model parameters w, Learning Rate (LR) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\eta$$\\end{document}\u03b7, size of batch b, and the number of layers in both CNN were configured accordingly. For the optimization of our model parameters, we have used the stochastic gradient descent algorithm (SGD). The pseudo-code of the proposed model is given in algorithm 1 and flow chart in Fig. 2.Figure 2Flow chart of proposed tumor classification framework in IoT healthcare systems. The pre trained CNN models (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) are trained with image-net dataset and the generated weights of these pre trained models are individually transferred to proposed CNN model for effective training. While the augmented data set is used for fine-tuning of the ResNet-CNN model for final classification of brain tumors.\nData set\n        We used a brain tumor data set (BTDS) from China\u2019s Nanfang hospital and general hospital, as well as Tianjing medical university, in this study (2005 to 2010)9, and new versions in 2017 have been published. T1-Weighted Contrast-Enhanced images (TWCEI) of 233 subjects with meningioma, glioma, and pituitary tumours are included in this data set. The data set is freely accessible via the Kaggle repository15. We also used the Brain MRI Images Data Set (BMIDS) for cross dataset validation, which contains 253 MRI brain images. The tumor class in the data set has 155 images, while the non-tumor class has 98 images16.\nBackground of convolutional neural network (CNN) architecture\n        Deep Learning model convolutional neural networks is a kind of Feed-Forward Neural Network17. Convolutions can capture translation invariance, which means that the filter is independent of position that significantly reduces the number of parameters. The CNN model have Convolutional, Pooling, and fully connected layers. Different functions are accomplished by these layers, such as dimensionality reduction, feature extractors, and classification. During the convolution operation of the forward pass, the filter is slide on the input shape and compute the map of activation, which computing the point-wise value of each output. Further add these output to achieve the activation of that point. Designed a Sliding Filter (SF) using convolution as a linear operator, and expressed as a dot product for fast deployment. Let consider x and w are input and the kernel function, the convolution process \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(x*w)(a)$$\\end{document}(x\u2217w)(a) on time index t can be mathematically expressed in Eq. (1).1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} (x*w) a=\\int x(t) w (a-t)da \\end{aligned}$$\\end{document}(x\u2217w)a=\u222bx(t)w(a-t)da\n        In Eq. (1) a is in \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{ R}^n$$\\end{document}Rn for any \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n \\ge 1$$\\end{document}n\u22651. While Parameter t is discrete. In this case, the discrete convolution can be expressed as in Eq. (2):2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} x.w (a) = \\sum _{a}x w (t-a) \\end{aligned}$$\\end{document}x.w(a)=\u2211axw(t-a)\n        However, usually use 2 or 3-dimensional convolutions in CNN model. In case of 2-dimensional image I as input and K is a two dimensional kernel and the convolution can be mathematically expressed as in Eq. (3):3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} (I*K)(i,j)=\\sum _{m}\\sum _{n}I(m,n)K(i-m,j-n) \\end{aligned}$$\\end{document}(I\u2217K)(i,j)=\u2211m\u2211nI(m,n)K(i-m,j-n)\n        If the case is 3 dimensional data image, then the convolution process can be written mathematically in Eq. (4) as follow:4\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} (I*K)(i,j,k)=\\sum _{m}\\sum _{n}\\sum _{l}I(m,n,l)K(i-m,j-n,k-l) \\end{aligned}$$\\end{document}(I\u2217K)(i,j,k)=\u2211m\u2211n\u2211lI(m,n,l)K(i-m,j-n,k-l)\n        In addition to gain non-linearities, two activation functions can be incorporate suc as Sigmoid and ReLU. The sigmoid activation fumction non-linearity is expressed mathematically in Eq. (5):5\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\theta (x)=\\frac{1}{1+exp(-x)}, x \\in R. \\end{aligned}$$\\end{document}\u03b8(x)=11+exp(-x),x\u2208R.\n        The sigmoid non-linearity activation function is suitable when need the output to be include in the range of [0,1]. Furthermore, the sigmoid function is monotone growing which means \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lim \\limits _{n \\rightarrow +\\infty } \\theta (x)=1$$\\end{document}limn\u2192+\u221e\u03b8(x)=1, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lim \\limits _{n \\rightarrow +\\infty } \\theta (x)=0$$\\end{document}limn\u2192+\u221e\u03b8(x)=0. However, this fact may be cause vanishing gradients, when the input x is not near to 0, the neuron will be more and the gradient of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta (x)$$\\end{document}\u03b8(x) will nearly to zero and will make successive optimization difficult.\n        The second activation function is relu which is mathematically defined in Eq. (6):6\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Relu(x) = max(0, x), x \\in R \\end{aligned}$$\\end{document}Relu(x)=max(0,x),x\u2208R\n        The gradient of of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$relu(x)=1$$\\end{document}relu(x)=1 for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x>0$$\\end{document}x>0 and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$relu^-(x)=0$$\\end{document}relu-(x)=0 for \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x<0$$\\end{document}x<0. The relu convergence capability of is good then sigmoid non-linearities.\n        The CNN model Pooling layers are utilized to produce a statistics summary of its inputs and deduced the dimensionality without missing important information. There are different types of pooling. In the layer of Max-Pooling generate the extreme values in individually rectangular neighborhood of individual point i.e i, j, k for data of three dimensional of individual feature of input respectively, while the average values generated by the average pooling layer.\n        The last layer is fully connected with n and m respectively input and output sizes. The output layer is expressed by the parameters such as a weight matrix i.e \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W \\in M_{m, n}$$\\end{document}W\u2208Mm,n with m rows, and n columns and a bias vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$b \\in {\\textbf {R}}^m$$\\end{document}b\u2208Rm. The input vector \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x \\in {\\textbf {R}}^n$$\\end{document}x\u2208Rn, the fully connected output layer FC along function of activation f is expressed mathematically in Eq. (7) as:7\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} FC(x): = f (Wx+b) \\in R^m \\end{aligned}$$\\end{document}FC(x):=f(Wx+b)\u2208Rm\n        In Eq. (7) Wx is the product matrix while the function f is used component wise.\n        The last layers fully connected employed for classification of problems. The CNN model architecture last layer is fully connected layers and CNN output is flattened and showed as a single vector.\nConvolution neural network for brain tumors classification\n        Recently, CNN models generated significant outcomes in numerous domains, such as NLP, image classification18, and diagnosis systems. In contrast to MLPs, CNN reduces the number of neurons and parameters, which results in lower complexity and faster adaptation.\n        The CNN model has significant applications in the classification of medical images18,19. In this paper we developed the CNN networks architecture with 4 alternating convolutional layers and max-pooling layers and a dropout layer after each Conv/pooling pair. The last pooling layer connected fully layer with 256 neurons, ReLU activation function, dropout layer, and sigmoid activation function are employed for classification of brain MR images (Meningioma, Glioma, and Pituitary). In addition, we have used the optimization algorithm Stochastic Gradient Descend (SGD)20. The CNN architecture is given in Fig. 1.Figure 1CNN model architecture for classification of Brain tumors.\nImprove CNN model for brain tumors classification\n        To improve CNN model predictive accuracy, we employed Data augmentation (DA) and Transfer learning (TL) techniques. The data augmentation can resolve the problem of insufficient data for model training. To expand the data amount, the zooming technique is used on original image data to produce images data with the similar label. The new created data set is used for fine tuning of the model. Th The transfer learning (TL) techniques widely used in image classification tasks21, cancer sub-type recognition22 and medical images filtering23. In this work, we used the transfer learning ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet models to enhanced the predictive performance of the proposed CNN model. The ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet pre-train models were trained on imageNet data set and transferred the trained parameters weights of these models individually to CNN model for effective training, and fine-tuned the model using the brain tumor augmented MR images data set for final classification of the CNN model.\nModel cross validation and evaluation criteria\n        The holdout cross-validation6,24,25 mechanism was used for training and validation of the model. In hold out CV data is randomly assign to two sets \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_0$$\\end{document}d0 and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_1$$\\end{document}d1. The \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_0$$\\end{document}d0 and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_1$$\\end{document}d1 use for training and testing of the model respectively. In hold out CV the training data set is usually large as compare to testing data set. The is train on \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_0$$\\end{document}d0 and testing on \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d_1$$\\end{document}d1. The holdout CV is suitable validation method in case when the data set is very plenty. In this study brain tumor MRI Images data set was divided into 70% for training and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$30\\%$$\\end{document}30% for teasing of the model. The performance evaluation metrics Accuracy (Acc), Sensitivity (Sn), Specificity (Sp), Precision (Pr), F1-Score (F1-S), and Matthews Correlation Coefficient (MCC)26\u201329 are used for model evaluation.\nProposed brain tumors classification model\n        NCNN models are now popular for image classification problems. A large image data set is more suitable for the CNN model\u2019s effective training, as it allows the model to extract more related features during the training process for accurate image classification. The CNN model\u2019s performance suffers as a result of the scarcity of large image data sets, particularly in the medical domain. However, to enhance the proposed CNN classifier performance, data augmentation and transfer learning6,21,30,31 techniques are incorporated. We have used transfer learning pre-trained models ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet along with data augmentation technique zooming. The imagesNet data set has been employed for pre-trained of ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet models, and the generated weights (trained parameters) of these models were transferred for the effective training of the CNN model individually. Brain tumor MRI data set was used for fine-tuning of CNN model and for final classification of the model in IoT healthcare system.\n        Furthermore, the proposed CNN model was trained and tested on a data set of brain tumour MR images, and its performance was compared to that of the transfer learning technique. A heldout cross-validation mechanism is used in the proposed method for model training and testing, with 70% used for training and 30% for model validation. The data augmentation20 technique was used to augment the original dataset by using the zooming method, which improves the model generalisation capability. The integration of data augmentation and transfer learning greatly enhanced the predictive accuracy of the CNN model. The evaluation criteria of the model different assessment metrics have used.\n        The data set X(i,\u00a0i) embedded into the CNN classifier,We used data transformations to increase the size of the data set so that we could train the model. Furthermore, the number of epochs E, model parameters w, Learning Rate (LR) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\eta$$\\end{document}\u03b7, size of batch b, and the number of layers in both CNN were configured accordingly. For the optimization of our model parameters, we have used the stochastic gradient descent algorithm (SGD). The pseudo-code of the proposed model is given in algorithm 1 and flow chart in Fig. 2.Figure 2Flow chart of proposed tumor classification framework in IoT healthcare systems. The pre trained CNN models (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) are trained with image-net dataset and the generated weights of these pre trained models are individually transferred to proposed CNN model for effective training. While the augmented data set is used for fine-tuning of the ResNet-CNN model for final classification of brain tumors.\nExperiments\n      \n        Experimental setup\n        We conducted various experiments to test the feasibility of our proposed model in IoT healthcare system. The proposed model was tested using a brain tumour image data set in this study. To improve the proposed CNN model predictive performance, we have employed (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) CNN pre trained models with imagenet dataset to produce high trained parameters (weights) and then transferred trained parameters weights of pre trained models to the CNN model individually for effective training of the model. For fine-tuning the CNN model, the brain tumor images data set was employed for final classification. The brain tumor data have 233 subjects and 3064 slices, which belong to three classes, i.e., Meningioma, Glioma, and Pituitary. This data set is very Small for effective training of the CNN model. In addition to tackle the problem of small brain tumor data method of data augmentation20 has used to augment the original data set. Data augmentation technique (zooming) is used, and all three types of images (Meningioma, Glioma, and Pituitary) are zoomed horizontally and vertically and added with existing images. The new augmented data set image size of three kinds of images is 6128. Held out technique is used for model training and validation, and respectively 70% and 30% data are employed for training and validation of the model for all experiments. To effectively optimize the model SGD Optimization algorithm is used20. In addition, other parameters such as learning rate (LR) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha$$\\end{document}\u03b1, SGD = 0.0001, epochs = 100, batch size = 120, outer and inner activation function = ReLu is used in all experiments. It is worth noting that for the final prediction layer our CNN model, the softmax activation function was used. Evaluation metrics are incorporated to evaluate the model performance.\n        All experiments used a laptop and a Google collaborator with GPU. All experiments required Python v3.7, and the CNN model was created using Keras framework v2.2.4 as a high-level API and Tensor flow v1.12 as the back end. All experiments were repeated numerous times to obtain consistent results. All experiment results were tabulated and graphed.\n      \n      \n        Results and analysis\n        \n          Results of data pre-processing\n          The brain tumor data set (BTDS) is obtained from the Kaggle repository15. T1-weighted contrast-enhanced images of 233 meningioma, glioma, and pituitary tumour patients are included in this data set. The Brain Tumor data contains 233 subjects and 3064 slices, with meningioma subjects accounting for 82 with slices 708, glioma subjects accounting for 91 with slices 1426, and pituitary subjects accounting for 60 with slices 930. Thus, the total number of subjects in the data is 233, and the total number of slices is 3064. In order to reduce the dimension of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$512\\times 512\\times 1$$\\end{document}512\u00d7512\u00d71 into \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$224\\times 224\\times 1$$\\end{document}224\u00d7224\u00d71 for effective training of model.\n          To handle imbalance problem in data set because Brain tumor data set has the different number of three subjects slices. The distribution of the data is different, and it creates a problem of over fitting the model. To balance the meningioma, glioma, and pictutitary in the data set, we incorporate the data augmentation20 method to augment the original dataset by using random zooming. All slices are being zoomed, and a new data set with 6128 slices has been created. The ratio of samples in an original data set is shown in Fig. 3. The data set has three subfolders for meningioma, glioma, and pictutitary images. Held out techniques is used for model training and validation because the new data set is very big and heldout validation is suitable in case of plenty dataset. The data set has splitted into 70% and 30% for training and validation of the model respectively. The cross-validation method has also been employed for an augmented data set.\n          We also used the Brain MRI Images Data Set (BMIDS) for cross dataset validation, which contains 253 MRI brain images. The tumor class in the data set has 155 images, while the non-tumor class has 98 images.Figure 3Ratio of samples in data set.\n        \n        \n          Results of the proposed CNN model, on original and augmented data sets\n          The performance of the proposed CNN model is evaluated using the original and augmented brain tumour MR image data sets. The CNN model is configured with essential hyper-parameters such as optimizer SGD with a Learning Rate (LR) of 00.0001, epochs 100, and size of batch was 120. The 70% data for training and 30% for the testing of the model is used. Different evaluation matrices were used for model performance evaluation. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 is used for training and evaluation of the proposed CNN model. All these hyper-parameters values and the output of the experimental results have been reported in Table 1.\n          Table 1 presented the proposed CNN model obtained 97.40% accuracy, 98.03% specificity, 95.10% sensitivity, 99.02% Precision, 97.75% MCC, and 97.26% F1-score respectively on original brain tumor MR images data set. The 97.40% accuracy demonstrated that our CNN architecture accurately classifies the three classes of brain tumors (meningioma, glioma, and pictutitary). The 98.03% specificity shows that the Proposed CNN model is a highly suitable detecting model for healthy subjects recognition, while 95.10% sensitivity presents that the model significantly detected the affected subjects. The MCC value was 97.75%, which gives confusion metrics a good summary.\n          On the other hand, the CNN model gained very excellent performance when trained and evaluated on an augmented data set. The CNN model obtained 98.56% accuracy, 100.00% specificity, 98.09% sensitivity, and 98.00% MCC when trained and evaluated on an augmented data set. The accuracy of the model improved from 97.40 to 98.56% which demonstrated the importance of the data augmentation process. Also, it illustrated that model needs more data for effective training of the CNN model.\n          From the experimental results, we concluded that the proposed CNN model effectively classified the brain tumor types, and the augmentation process further improved the model CNN performance because the CNN model more data for extract more related features for classification. The high accuracy of the proposed CNN model might be due to the suitable architecture of the CNN model and proper fitting of essential parameters of the model and data augmentation.Table 1CNN model performance on original and augmented data sets.Data setParametersAssessment metricsOptimizerLRAcc (%)Sp (%)Sn (%)Pr (%)MCC (%)F1-S (%)OriginalSGD0.000197.4098.0395.1099.0297.7597.26Augmented\u2013\u201398.56100.0098.0997.1298.0098.10\n        \n        \n          CNN model performance evaluation with cross dataset\n          We have evaluated the predictive performance of CNN model with independent cross dataset. We trained the proposed CNN model with original and augmented brain tumor data set and validated with independent Brain MRI Images Data Set (BMIDS). The model is configured with essential hyper-parameters such as optimizer SGD with a Learning Rate (LR) of 00.0001, epochs 100, and size of batch was 120. Different evaluation matrices were used for model performance evaluation. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 is used for training and evaluation of the proposed CNN model. The experimental results of model with cross data are reported in Table 2.\n          Table 2 presented that the proposed CNN model obtained 97.96% accuracy, 99.00% specificity, 97.30% sensitivity, 98.18% Precision, 98.00% MCC, and 99.02% F1-score when trained on original brain tumor MR images data set (BTDS) and validated with independent data set (BMIDS).\n          Other other side the model achieved 98.97% accuracy, 99.89% specificity, 99.39% sensitivity, 98.89% Precision, 99.40% MCC, and 99.30% F1-score when trained with augmented data set (BTDS) and validated with independent data set (BMIDS). Hence, from experimental results we observed that model predictive and generalization capability improved when trained and validated with independent data sets.Table 2CNN model performance with cross data set.Data setParametersAssessment metricsOptimizerLRAcc (%)Sp (%)Sn (%)Pr (%)MCC (%)F1-S (%)OriginalSGD0.000197.9699.0097.3098.1898.0099.02Augmented\u2013\u201398.9799.8999.3998.8999.4099.30\n        \n        \n          Results of the transfer learning models (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) on original and augmented data sets\n          The performances of transfer learning (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) models have checked on original and augmented data sets. Theses models have configured other essential hyper-parameters such as optimizer SGD with learning rate 0.0001, the number of epoch 100, batch size 120. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 is used for training and evaluation of the proposed model. The (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) models are evaluated using different performance evaluation metric. The (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) models hyper-parameters values and the output of the experimental results have reported in Table 3.Table 3Transfer learning models predictive performance on original and augmented data sets.ModelData setSpace complexityTime complexityLRAssessment metrics\u201325.6M3.2\u00a0h0.0001Acc (%)Sp (%)Sn (%)Pr (%)MCC (%)F1-S (%)ResNet50Original0.000197.0397.0493.1094.2193.2395.00Augmented\u201398.0799.30100.0096.0796.0097.00VGG-16\u2013138.4M3.02\u00a0h\u201394.7796.3094.6793.4391.9096.61\u2013\u201395.9796.9599.4096.8492.9896.80Inception V3\u201323.9M3.96\u00a0h\u201393.2396.8995.0096.0895.5697.87\u2013\u201396.0397.0397.0097.0196.0598.00DenseNet201\u201320.2M3.9\u00a0h\u201396.7695.9995.6095.7898.4598.23\u2013\u201397.4397.7899.3096.6198.4498.89Xception\u201322.9M4.3\u00a0h\u201393.0097.0398.0097.0999.3297.23\u2013\u201395.6098.9896.0098.0499.9898.00MobilleNet\u20134.3M2.6\u00a0h\u201396.7698.0999.5095.9896.6498.23\u2013\u201397.8799.0094.5696.2398.4597.89The space complexity of each model is the number of trainable parameters. M\u00a0= Million. The space complexity increases with increasing number of trainable parameters. The time complexity is the training time (in hours) of the models.\n          Table 3 show that the ResNet-50 model obtained 97.03% accuracy, 97.04% specificity, 93.10% sensitivity, 94.21% Precision, 93.23% MCC, and 95.00% F1-score respectively on original brain tumor data set. The 95.30% accuracy show that the ResNet-50 model accurately classifies the three classes of brain tumors (meningioma, glioma, and pictutitary). The 97.04% specificity shows that the ResNet-50 model is a highly suitable detecting model for healthy subjects recognition, while 93.10% sensitivity show that the model accurately detected the affected subjects.\n          The predictive Performance of transfer learning model ResNet-50 very high when model trained and evaluated with augmented data set. According to Table 3 the transfer learning model ResNet-50 obtained 98.07% accuracy, 99.30% specificity, 100.00% sensitivity, 96.07% precision, 96.00% MCC, and 97.00% F1-S, when trained and evaluated on augmented data set.\n          The VGG-16 model with original and augmented data sets obtained 94.77% accuracy, 96.30% specificity, 94.67% sensitivity, 93.43% precision, 91.90% MCC, 96.61% F1-S, and 95.97% accuracy, 96.95% specificity, 99.40% sensitivity, 96.84% precision, 92.98% MCC, and 96.80% F1-S respectively.\n          Inception V3 obtained 93.23% accuracy, 96.89% specificity, 95.00% sensitivity, 96.08% precision, 95.56% MCC, and 97.87% F1-s, with original data set. While on augmented data set Inception V3 obtained 96.03% accuracy, 97.03% specificity, 97.00% sensitivity, 97.01% precision, 96.05% MCC, 98.00% F1-S. DenseNet201 model obtained 96.76% accuracy on original data set and increase it 97.43% accuracy with augmented data set.\n          The Xception model with original data set achieved 93.00% accuracy, 97.03% specificity, 98.00% sensitivity, 97.09% precision, 99.32% MCC, 97.23% F1-S and obtained 95.60% accuracy, 98.98% specificity, 96.00% sensitivity, 98.04% precision, 99.98% MCC, and 98.00% F1-S with augmented data set. MobilleNet model obtained 96.76% accuracy with original data set and 97.87% with augmented data set. Among all models the ResNet-50 model performance in terms of accuracy is high with augmented data set. The model improved accuracy from 95.30 to 98.07% with data augmentation. The other evaluation metrics values also improved with data augmentation. From the experimental results, we concluded that the data augmentation process increased the training of ResNet-50 and model effectively classified the brain tumor types.\n        \n        \n          Results of the integrated frameworks (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) on original and augmented data sets\n          The integrated frameworks (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) performances have checked on original and augmented data sets. Furthermore, we have incorporated the TL ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet CNN architectures with imageNet data set to generate high weights and then transferred trained parameters weights of these pre trained models to the CNN model individually for effective training of CNN model. For fine-tuning of the CNN model, the brain tumors original and augmented data sets have used for final classification. The models have configured with concern hyper-parameters such as optimizer SGD with learning rate 0.0001, the number of epoch 100, batch size 120. The proposed framework performance has been evaluated employing various matrices. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 has been used for training and evaluation of the proposed model. All these hyper-parameters values and the output of the experimental results of (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) models have reported in Table 4.\n          Table 4 presented that the ResNet50-CNN model obtained 99.10% accuracy, 100.00% specificity, 89.60% sensitivity, 98.75% Precision, 98.66% MCC, and 99.5% F1-score respectively on original brain tumor data set. The 99.10% accuracy demonstrated that the architecture accurately classifies the three classes of brain tumors (meningioma, glioma, and pictutitary). The 100% specificity shows that the Proposed model is a highly suitable detecting model for healthy subjects recognition, while 89.60% sensitivity presents that the model significantly detected the affected subjects.\n          On the other hand, the model obtained very high performance when it trained and evaluated on the augmented data set. The integrated CNN and transfer learning model (ResNet-50-CNN) obtained 99.90% accuracy, 99.08% specificity, 96.13% sensitivity, and 99.10% MCC when trained and evaluated on augmented data set.\n          The VGG-16-CNN model with original and augmented data sets obtained 96.78% accuracy, 99.23% specificity, 95.00% sensitivity, 96.99% precision, 98.93% MCC, 97.98% F1-S, and 97.88% accuracy, 98.00% specificity, 100.00% sensitivity, 96.98% precision, 98.79% MCC, and 99.00% F1-S respectively.\n          Inception V3-CNN model obtained 97.00% accuracy, 99.00% specificity, 99.87% sensitivity, 98.92% precision, 95.76% MCC, 98.09% F1-S with original data set. While on augmented data set Inception V3 obtained 98.02% accuracy, 100.00% specificity, 98.67% sensitivity, 97.56% precision, 99.00% MCC, and 97.30% F1-S.\n          DenseNet201-CNN model obtained 97.00% accuracy on original data set and increase it 97.90% accuracy with augmented data set. Hence, the integrated model DenseNet201-CNN improved accuracy 97.00\u201397.90%\u00a0= 0.90% with data augmentation process.\n          The Xception-CNN model with original data set achieved 98.20% accuracy, 98.88% specificity, 97.40% sensitivity, 99.00% precision, 99.10% MCC, 98.65% F1-S, and obtained 98.97% accuracy, 99.00% specificity, 98.60% sensitivity, 97.24% precision, 97.99% MCC, 99.30% F1-S with augmented data set. MobilleNet-CNN model obtained 98.08% accuracy with original data set and 98.56% with augmented data set. The improved accuracy 98.08% to 98.56% when model fine tuned with augmented data set.\n          From above anlaysis we conculded that among all the ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN, the predictive performance of ResNet-50-CNN model is high in terms of accuracy. The accuracy of the model improved from 99.10 to 99.90% which is illustrated the importance of the data augmentation and transfer learning process. Hence we concluded that the ResNet-50-CNN model effectively classify the brain tumor types. The high accuracy of the proposed integrated diagnosis framework might be due to the suitable architecture of the model and proper fitting of essential parameters of the model and data augmentation. In addition, the proposed integrated model (ResNet-50-CNN) accuracy has compared with CNN model and transfer learning ResNet-50 model in Table 5 on augmented data set and graphically shown in Fig. 4.Table 4Integrated frameworks (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) performance on original and augmented data sets.ModelData setSpace complexityTime complexityLRAssessment metrics\u201325.6M3.3h0.0001Acc(%)Sp(%)Sn(%)Pr(%)MCC(%)F1-S(%)ResNet50-CNNOriginal0.000199.10100.0089.6098.7598.6699.5Augmented\u201399.9099.0896.1399.2299.1099.43VGG-16-CNN\u2013138.4M3.21\u00a0h\u201396.7899.2395.0096.9998.9397.98\u2013\u201397.8898.00100.0096.9898.7999.00Inception V3-CNN\u201323.9M4.13\u00a0h\u201397.0099.0099.8798.9295.7698.09\u2013\u201398.02100.0098.6797.5699.0097.30DenseNet201-CNB\u201320.2M4.06\u00a0h\u201397.0099.9998.1199.1297.9899.09\u2013\u201397.90100.0097.4595.6899.0399.32Xception-CNN\u201322.9M4.55\u00a0h\u201398.2098.8897.4099.0099.1098.65\u2013\u201398.9799.0098.6097.2497.9999.30MobilleNet-CNN\u20134.3M2.7\u00a0h\u201398.0899.4393.2099.1297.6593.231\u2013\u201398.56100.0099.7693.9899.3299.05The space complexity of each model is the number of trainable parameters. M\u00a0= Million. The space complexity increases with increasing number of trainable parameters. The time complexity is the training time (in hours) of the models.Table 5Accuracy of CNN, ResNet-50 and ResNet-50-CNN on augmented data.CNN (Acc%)ResNet-50 (Acc%)ResNet-50-CNN (Acc%)98.9798.0799.90\n          \nFigure 4CNN, ResNet-50 and Integrated (ResNet-50-CNN) models accuracy comparison with augmented Brain tumor data set. The CNN model obtained accuracy\u2019s with augmented data is 98.97%, while ResNet-50 obtained 96.07% and Integrated model ResNet-CNN obtained high predictive accuracy 99.90% with augmented data. Thus, the proposed integrated ResNet-CNN model is suitable for effective classification of brain tumors and could assist clinical professionals to diagnosis brain cancer accurately and efficiently. Due to the high performance of proposed ResNet-CNN method we recommend it for diagnosis of brain cancer in IoT-healthcare.\n\n        \n        \n          Accuracy comparison of the proposed (ResNet-CNN) model with state of-the-art models\n          We have compared our ResNet-50-CNN (ResNet-CNN) model performance in terms of accuracy with state-of-the-art methods in Table 6. Table 6 and Fig. 5 presented that proposed model obtained 99.89% accuracy, which is high as compared to state-of-the-art techniques. The high performance of the proposed method demonstrated that it is correctly classified brain tumors (meningioma, glioma, and pictutitary), and it can easily be deployed in IoT-health care for the classification of brain tumors.Table 6Comparison of ResNet-CNN model accuracy with previous models.ModelAcc (%)RefSpace complexityTime cmplexityANN and KNN97, 9810\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((nm + mk) + (n*d) )$$\\end{document}O((nm+mk)+(n\u2217d))\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((cwh) + (nd+Kn))$$\\end{document}O((cwh)+(nd+Kn))GA-CNN94.212\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)SVM and KNN85, 888\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((n) + (n*d))$$\\end{document}O((n)+(n\u2217d))\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((n^2) + (nd+Kn))$$\\end{document}O((n2)+(nd+Kn))CNN-TF94.8213\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)Proposed method ResNet-CNN99.902022\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c=$$\\end{document}c= the number of convolutional channels, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h=$$\\end{document}h=\u00a0height of input, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w=$$\\end{document}w= width of input, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f=$$\\end{document}f= the convolutional kernel size, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n=$$\\end{document}n= the number data instances, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k=$$\\end{document}k= the number of output neurons, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m=$$\\end{document}m= the number of input neurons and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d =$$\\end{document}d= the dimension or feature of the input, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$K=$$\\end{document}K=\u00a0number of nearest neighbors, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u=c*w*h$$\\end{document}u=c\u2217w\u2217h.Figure 5ResNet-CNN model performance comparison with baseline models show that our model predictive performance in terms of accuracy is high from baseline models. The ResNet-CNN model cloud accurately and efficiently classify the brain tumors and assist medical experts to interpret the images of brain tumors to diagnosis brain cancer.\n        \n        \n          Space and time complexity\n          Also, in Tables 3, 4, and 6, we present both the models space and complexity of the various proposed methods used in the prediction of Brain cancer. Since the proposed models are convolutional deep learning methods, the space complexities are analyzed in terms of the each model\u2019s trainable parameters. For the time complexity, the model\u2019s training time is used. It could be deduced from Table 3 that VGG-16 has the worst space complexity since its trainable parameter is 138.4 million, whiles MobileNet has the best space time complexity. Moreover for the time complexity, the Xception model has the worst time complexity because its training time is 4.3\u00a0h. Because of the difficulty of accessing the models of the competing methods in Table 4 , we could not experimentally analyze the complexity of the models in terms of algorithmic run-time. It is more likely that almost all the methods with the deep learning techniques, the convolutional neural networks will have a worse space and time complexity because of the significant number of parameters and matrix computation that come with the models\u2019 architecture. Irrespective of the worst case time and space complexity, our proposed model has an accuracy performance gain as compared to all competing methods. The time complexity is the training time (in hours) of the models as reported in Tables 3, 4, and 6. The space and time complexity of our model are \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m) respectively.\nExperimental setup\n        We conducted various experiments to test the feasibility of our proposed model in IoT healthcare system. The proposed model was tested using a brain tumour image data set in this study. To improve the proposed CNN model predictive performance, we have employed (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) CNN pre trained models with imagenet dataset to produce high trained parameters (weights) and then transferred trained parameters weights of pre trained models to the CNN model individually for effective training of the model. For fine-tuning the CNN model, the brain tumor images data set was employed for final classification. The brain tumor data have 233 subjects and 3064 slices, which belong to three classes, i.e., Meningioma, Glioma, and Pituitary. This data set is very Small for effective training of the CNN model. In addition to tackle the problem of small brain tumor data method of data augmentation20 has used to augment the original data set. Data augmentation technique (zooming) is used, and all three types of images (Meningioma, Glioma, and Pituitary) are zoomed horizontally and vertically and added with existing images. The new augmented data set image size of three kinds of images is 6128. Held out technique is used for model training and validation, and respectively 70% and 30% data are employed for training and validation of the model for all experiments. To effectively optimize the model SGD Optimization algorithm is used20. In addition, other parameters such as learning rate (LR) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha$$\\end{document}\u03b1, SGD = 0.0001, epochs = 100, batch size = 120, outer and inner activation function = ReLu is used in all experiments. It is worth noting that for the final prediction layer our CNN model, the softmax activation function was used. Evaluation metrics are incorporated to evaluate the model performance.\n        All experiments used a laptop and a Google collaborator with GPU. All experiments required Python v3.7, and the CNN model was created using Keras framework v2.2.4 as a high-level API and Tensor flow v1.12 as the back end. All experiments were repeated numerous times to obtain consistent results. All experiment results were tabulated and graphed.\nResults and analysis\n        \n          Results of data pre-processing\n          The brain tumor data set (BTDS) is obtained from the Kaggle repository15. T1-weighted contrast-enhanced images of 233 meningioma, glioma, and pituitary tumour patients are included in this data set. The Brain Tumor data contains 233 subjects and 3064 slices, with meningioma subjects accounting for 82 with slices 708, glioma subjects accounting for 91 with slices 1426, and pituitary subjects accounting for 60 with slices 930. Thus, the total number of subjects in the data is 233, and the total number of slices is 3064. In order to reduce the dimension of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$512\\times 512\\times 1$$\\end{document}512\u00d7512\u00d71 into \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$224\\times 224\\times 1$$\\end{document}224\u00d7224\u00d71 for effective training of model.\n          To handle imbalance problem in data set because Brain tumor data set has the different number of three subjects slices. The distribution of the data is different, and it creates a problem of over fitting the model. To balance the meningioma, glioma, and pictutitary in the data set, we incorporate the data augmentation20 method to augment the original dataset by using random zooming. All slices are being zoomed, and a new data set with 6128 slices has been created. The ratio of samples in an original data set is shown in Fig. 3. The data set has three subfolders for meningioma, glioma, and pictutitary images. Held out techniques is used for model training and validation because the new data set is very big and heldout validation is suitable in case of plenty dataset. The data set has splitted into 70% and 30% for training and validation of the model respectively. The cross-validation method has also been employed for an augmented data set.\n          We also used the Brain MRI Images Data Set (BMIDS) for cross dataset validation, which contains 253 MRI brain images. The tumor class in the data set has 155 images, while the non-tumor class has 98 images.Figure 3Ratio of samples in data set.\n        \n        \n          Results of the proposed CNN model, on original and augmented data sets\n          The performance of the proposed CNN model is evaluated using the original and augmented brain tumour MR image data sets. The CNN model is configured with essential hyper-parameters such as optimizer SGD with a Learning Rate (LR) of 00.0001, epochs 100, and size of batch was 120. The 70% data for training and 30% for the testing of the model is used. Different evaluation matrices were used for model performance evaluation. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 is used for training and evaluation of the proposed CNN model. All these hyper-parameters values and the output of the experimental results have been reported in Table 1.\n          Table 1 presented the proposed CNN model obtained 97.40% accuracy, 98.03% specificity, 95.10% sensitivity, 99.02% Precision, 97.75% MCC, and 97.26% F1-score respectively on original brain tumor MR images data set. The 97.40% accuracy demonstrated that our CNN architecture accurately classifies the three classes of brain tumors (meningioma, glioma, and pictutitary). The 98.03% specificity shows that the Proposed CNN model is a highly suitable detecting model for healthy subjects recognition, while 95.10% sensitivity presents that the model significantly detected the affected subjects. The MCC value was 97.75%, which gives confusion metrics a good summary.\n          On the other hand, the CNN model gained very excellent performance when trained and evaluated on an augmented data set. The CNN model obtained 98.56% accuracy, 100.00% specificity, 98.09% sensitivity, and 98.00% MCC when trained and evaluated on an augmented data set. The accuracy of the model improved from 97.40 to 98.56% which demonstrated the importance of the data augmentation process. Also, it illustrated that model needs more data for effective training of the CNN model.\n          From the experimental results, we concluded that the proposed CNN model effectively classified the brain tumor types, and the augmentation process further improved the model CNN performance because the CNN model more data for extract more related features for classification. The high accuracy of the proposed CNN model might be due to the suitable architecture of the CNN model and proper fitting of essential parameters of the model and data augmentation.Table 1CNN model performance on original and augmented data sets.Data setParametersAssessment metricsOptimizerLRAcc (%)Sp (%)Sn (%)Pr (%)MCC (%)F1-S (%)OriginalSGD0.000197.4098.0395.1099.0297.7597.26Augmented\u2013\u201398.56100.0098.0997.1298.0098.10\n        \n        \n          CNN model performance evaluation with cross dataset\n          We have evaluated the predictive performance of CNN model with independent cross dataset. We trained the proposed CNN model with original and augmented brain tumor data set and validated with independent Brain MRI Images Data Set (BMIDS). The model is configured with essential hyper-parameters such as optimizer SGD with a Learning Rate (LR) of 00.0001, epochs 100, and size of batch was 120. Different evaluation matrices were used for model performance evaluation. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 is used for training and evaluation of the proposed CNN model. The experimental results of model with cross data are reported in Table 2.\n          Table 2 presented that the proposed CNN model obtained 97.96% accuracy, 99.00% specificity, 97.30% sensitivity, 98.18% Precision, 98.00% MCC, and 99.02% F1-score when trained on original brain tumor MR images data set (BTDS) and validated with independent data set (BMIDS).\n          Other other side the model achieved 98.97% accuracy, 99.89% specificity, 99.39% sensitivity, 98.89% Precision, 99.40% MCC, and 99.30% F1-score when trained with augmented data set (BTDS) and validated with independent data set (BMIDS). Hence, from experimental results we observed that model predictive and generalization capability improved when trained and validated with independent data sets.Table 2CNN model performance with cross data set.Data setParametersAssessment metricsOptimizerLRAcc (%)Sp (%)Sn (%)Pr (%)MCC (%)F1-S (%)OriginalSGD0.000197.9699.0097.3098.1898.0099.02Augmented\u2013\u201398.9799.8999.3998.8999.4099.30\n        \n        \n          Results of the transfer learning models (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) on original and augmented data sets\n          The performances of transfer learning (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) models have checked on original and augmented data sets. Theses models have configured other essential hyper-parameters such as optimizer SGD with learning rate 0.0001, the number of epoch 100, batch size 120. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 is used for training and evaluation of the proposed model. The (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) models are evaluated using different performance evaluation metric. The (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) models hyper-parameters values and the output of the experimental results have reported in Table 3.Table 3Transfer learning models predictive performance on original and augmented data sets.ModelData setSpace complexityTime complexityLRAssessment metrics\u201325.6M3.2\u00a0h0.0001Acc (%)Sp (%)Sn (%)Pr (%)MCC (%)F1-S (%)ResNet50Original0.000197.0397.0493.1094.2193.2395.00Augmented\u201398.0799.30100.0096.0796.0097.00VGG-16\u2013138.4M3.02\u00a0h\u201394.7796.3094.6793.4391.9096.61\u2013\u201395.9796.9599.4096.8492.9896.80Inception V3\u201323.9M3.96\u00a0h\u201393.2396.8995.0096.0895.5697.87\u2013\u201396.0397.0397.0097.0196.0598.00DenseNet201\u201320.2M3.9\u00a0h\u201396.7695.9995.6095.7898.4598.23\u2013\u201397.4397.7899.3096.6198.4498.89Xception\u201322.9M4.3\u00a0h\u201393.0097.0398.0097.0999.3297.23\u2013\u201395.6098.9896.0098.0499.9898.00MobilleNet\u20134.3M2.6\u00a0h\u201396.7698.0999.5095.9896.6498.23\u2013\u201397.8799.0094.5696.2398.4597.89The space complexity of each model is the number of trainable parameters. M\u00a0= Million. The space complexity increases with increasing number of trainable parameters. The time complexity is the training time (in hours) of the models.\n          Table 3 show that the ResNet-50 model obtained 97.03% accuracy, 97.04% specificity, 93.10% sensitivity, 94.21% Precision, 93.23% MCC, and 95.00% F1-score respectively on original brain tumor data set. The 95.30% accuracy show that the ResNet-50 model accurately classifies the three classes of brain tumors (meningioma, glioma, and pictutitary). The 97.04% specificity shows that the ResNet-50 model is a highly suitable detecting model for healthy subjects recognition, while 93.10% sensitivity show that the model accurately detected the affected subjects.\n          The predictive Performance of transfer learning model ResNet-50 very high when model trained and evaluated with augmented data set. According to Table 3 the transfer learning model ResNet-50 obtained 98.07% accuracy, 99.30% specificity, 100.00% sensitivity, 96.07% precision, 96.00% MCC, and 97.00% F1-S, when trained and evaluated on augmented data set.\n          The VGG-16 model with original and augmented data sets obtained 94.77% accuracy, 96.30% specificity, 94.67% sensitivity, 93.43% precision, 91.90% MCC, 96.61% F1-S, and 95.97% accuracy, 96.95% specificity, 99.40% sensitivity, 96.84% precision, 92.98% MCC, and 96.80% F1-S respectively.\n          Inception V3 obtained 93.23% accuracy, 96.89% specificity, 95.00% sensitivity, 96.08% precision, 95.56% MCC, and 97.87% F1-s, with original data set. While on augmented data set Inception V3 obtained 96.03% accuracy, 97.03% specificity, 97.00% sensitivity, 97.01% precision, 96.05% MCC, 98.00% F1-S. DenseNet201 model obtained 96.76% accuracy on original data set and increase it 97.43% accuracy with augmented data set.\n          The Xception model with original data set achieved 93.00% accuracy, 97.03% specificity, 98.00% sensitivity, 97.09% precision, 99.32% MCC, 97.23% F1-S and obtained 95.60% accuracy, 98.98% specificity, 96.00% sensitivity, 98.04% precision, 99.98% MCC, and 98.00% F1-S with augmented data set. MobilleNet model obtained 96.76% accuracy with original data set and 97.87% with augmented data set. Among all models the ResNet-50 model performance in terms of accuracy is high with augmented data set. The model improved accuracy from 95.30 to 98.07% with data augmentation. The other evaluation metrics values also improved with data augmentation. From the experimental results, we concluded that the data augmentation process increased the training of ResNet-50 and model effectively classified the brain tumor types.\n        \n        \n          Results of the integrated frameworks (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) on original and augmented data sets\n          The integrated frameworks (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) performances have checked on original and augmented data sets. Furthermore, we have incorporated the TL ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet CNN architectures with imageNet data set to generate high weights and then transferred trained parameters weights of these pre trained models to the CNN model individually for effective training of CNN model. For fine-tuning of the CNN model, the brain tumors original and augmented data sets have used for final classification. The models have configured with concern hyper-parameters such as optimizer SGD with learning rate 0.0001, the number of epoch 100, batch size 120. The proposed framework performance has been evaluated employing various matrices. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 has been used for training and evaluation of the proposed model. All these hyper-parameters values and the output of the experimental results of (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) models have reported in Table 4.\n          Table 4 presented that the ResNet50-CNN model obtained 99.10% accuracy, 100.00% specificity, 89.60% sensitivity, 98.75% Precision, 98.66% MCC, and 99.5% F1-score respectively on original brain tumor data set. The 99.10% accuracy demonstrated that the architecture accurately classifies the three classes of brain tumors (meningioma, glioma, and pictutitary). The 100% specificity shows that the Proposed model is a highly suitable detecting model for healthy subjects recognition, while 89.60% sensitivity presents that the model significantly detected the affected subjects.\n          On the other hand, the model obtained very high performance when it trained and evaluated on the augmented data set. The integrated CNN and transfer learning model (ResNet-50-CNN) obtained 99.90% accuracy, 99.08% specificity, 96.13% sensitivity, and 99.10% MCC when trained and evaluated on augmented data set.\n          The VGG-16-CNN model with original and augmented data sets obtained 96.78% accuracy, 99.23% specificity, 95.00% sensitivity, 96.99% precision, 98.93% MCC, 97.98% F1-S, and 97.88% accuracy, 98.00% specificity, 100.00% sensitivity, 96.98% precision, 98.79% MCC, and 99.00% F1-S respectively.\n          Inception V3-CNN model obtained 97.00% accuracy, 99.00% specificity, 99.87% sensitivity, 98.92% precision, 95.76% MCC, 98.09% F1-S with original data set. While on augmented data set Inception V3 obtained 98.02% accuracy, 100.00% specificity, 98.67% sensitivity, 97.56% precision, 99.00% MCC, and 97.30% F1-S.\n          DenseNet201-CNN model obtained 97.00% accuracy on original data set and increase it 97.90% accuracy with augmented data set. Hence, the integrated model DenseNet201-CNN improved accuracy 97.00\u201397.90%\u00a0= 0.90% with data augmentation process.\n          The Xception-CNN model with original data set achieved 98.20% accuracy, 98.88% specificity, 97.40% sensitivity, 99.00% precision, 99.10% MCC, 98.65% F1-S, and obtained 98.97% accuracy, 99.00% specificity, 98.60% sensitivity, 97.24% precision, 97.99% MCC, 99.30% F1-S with augmented data set. MobilleNet-CNN model obtained 98.08% accuracy with original data set and 98.56% with augmented data set. The improved accuracy 98.08% to 98.56% when model fine tuned with augmented data set.\n          From above anlaysis we conculded that among all the ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN, the predictive performance of ResNet-50-CNN model is high in terms of accuracy. The accuracy of the model improved from 99.10 to 99.90% which is illustrated the importance of the data augmentation and transfer learning process. Hence we concluded that the ResNet-50-CNN model effectively classify the brain tumor types. The high accuracy of the proposed integrated diagnosis framework might be due to the suitable architecture of the model and proper fitting of essential parameters of the model and data augmentation. In addition, the proposed integrated model (ResNet-50-CNN) accuracy has compared with CNN model and transfer learning ResNet-50 model in Table 5 on augmented data set and graphically shown in Fig. 4.Table 4Integrated frameworks (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) performance on original and augmented data sets.ModelData setSpace complexityTime complexityLRAssessment metrics\u201325.6M3.3h0.0001Acc(%)Sp(%)Sn(%)Pr(%)MCC(%)F1-S(%)ResNet50-CNNOriginal0.000199.10100.0089.6098.7598.6699.5Augmented\u201399.9099.0896.1399.2299.1099.43VGG-16-CNN\u2013138.4M3.21\u00a0h\u201396.7899.2395.0096.9998.9397.98\u2013\u201397.8898.00100.0096.9898.7999.00Inception V3-CNN\u201323.9M4.13\u00a0h\u201397.0099.0099.8798.9295.7698.09\u2013\u201398.02100.0098.6797.5699.0097.30DenseNet201-CNB\u201320.2M4.06\u00a0h\u201397.0099.9998.1199.1297.9899.09\u2013\u201397.90100.0097.4595.6899.0399.32Xception-CNN\u201322.9M4.55\u00a0h\u201398.2098.8897.4099.0099.1098.65\u2013\u201398.9799.0098.6097.2497.9999.30MobilleNet-CNN\u20134.3M2.7\u00a0h\u201398.0899.4393.2099.1297.6593.231\u2013\u201398.56100.0099.7693.9899.3299.05The space complexity of each model is the number of trainable parameters. M\u00a0= Million. The space complexity increases with increasing number of trainable parameters. The time complexity is the training time (in hours) of the models.Table 5Accuracy of CNN, ResNet-50 and ResNet-50-CNN on augmented data.CNN (Acc%)ResNet-50 (Acc%)ResNet-50-CNN (Acc%)98.9798.0799.90\n          \nFigure 4CNN, ResNet-50 and Integrated (ResNet-50-CNN) models accuracy comparison with augmented Brain tumor data set. The CNN model obtained accuracy\u2019s with augmented data is 98.97%, while ResNet-50 obtained 96.07% and Integrated model ResNet-CNN obtained high predictive accuracy 99.90% with augmented data. Thus, the proposed integrated ResNet-CNN model is suitable for effective classification of brain tumors and could assist clinical professionals to diagnosis brain cancer accurately and efficiently. Due to the high performance of proposed ResNet-CNN method we recommend it for diagnosis of brain cancer in IoT-healthcare.\n\n        \n        \n          Accuracy comparison of the proposed (ResNet-CNN) model with state of-the-art models\n          We have compared our ResNet-50-CNN (ResNet-CNN) model performance in terms of accuracy with state-of-the-art methods in Table 6. Table 6 and Fig. 5 presented that proposed model obtained 99.89% accuracy, which is high as compared to state-of-the-art techniques. The high performance of the proposed method demonstrated that it is correctly classified brain tumors (meningioma, glioma, and pictutitary), and it can easily be deployed in IoT-health care for the classification of brain tumors.Table 6Comparison of ResNet-CNN model accuracy with previous models.ModelAcc (%)RefSpace complexityTime cmplexityANN and KNN97, 9810\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((nm + mk) + (n*d) )$$\\end{document}O((nm+mk)+(n\u2217d))\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((cwh) + (nd+Kn))$$\\end{document}O((cwh)+(nd+Kn))GA-CNN94.212\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)SVM and KNN85, 888\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((n) + (n*d))$$\\end{document}O((n)+(n\u2217d))\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((n^2) + (nd+Kn))$$\\end{document}O((n2)+(nd+Kn))CNN-TF94.8213\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)Proposed method ResNet-CNN99.902022\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c=$$\\end{document}c= the number of convolutional channels, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h=$$\\end{document}h=\u00a0height of input, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w=$$\\end{document}w= width of input, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f=$$\\end{document}f= the convolutional kernel size, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n=$$\\end{document}n= the number data instances, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k=$$\\end{document}k= the number of output neurons, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m=$$\\end{document}m= the number of input neurons and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d =$$\\end{document}d= the dimension or feature of the input, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$K=$$\\end{document}K=\u00a0number of nearest neighbors, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u=c*w*h$$\\end{document}u=c\u2217w\u2217h.Figure 5ResNet-CNN model performance comparison with baseline models show that our model predictive performance in terms of accuracy is high from baseline models. The ResNet-CNN model cloud accurately and efficiently classify the brain tumors and assist medical experts to interpret the images of brain tumors to diagnosis brain cancer.\n        \n        \n          Space and time complexity\n          Also, in Tables 3, 4, and 6, we present both the models space and complexity of the various proposed methods used in the prediction of Brain cancer. Since the proposed models are convolutional deep learning methods, the space complexities are analyzed in terms of the each model\u2019s trainable parameters. For the time complexity, the model\u2019s training time is used. It could be deduced from Table 3 that VGG-16 has the worst space complexity since its trainable parameter is 138.4 million, whiles MobileNet has the best space time complexity. Moreover for the time complexity, the Xception model has the worst time complexity because its training time is 4.3\u00a0h. Because of the difficulty of accessing the models of the competing methods in Table 4 , we could not experimentally analyze the complexity of the models in terms of algorithmic run-time. It is more likely that almost all the methods with the deep learning techniques, the convolutional neural networks will have a worse space and time complexity because of the significant number of parameters and matrix computation that come with the models\u2019 architecture. Irrespective of the worst case time and space complexity, our proposed model has an accuracy performance gain as compared to all competing methods. The time complexity is the training time (in hours) of the models as reported in Tables 3, 4, and 6. The space and time complexity of our model are \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m) respectively.\nResults of data pre-processing\n          The brain tumor data set (BTDS) is obtained from the Kaggle repository15. T1-weighted contrast-enhanced images of 233 meningioma, glioma, and pituitary tumour patients are included in this data set. The Brain Tumor data contains 233 subjects and 3064 slices, with meningioma subjects accounting for 82 with slices 708, glioma subjects accounting for 91 with slices 1426, and pituitary subjects accounting for 60 with slices 930. Thus, the total number of subjects in the data is 233, and the total number of slices is 3064. In order to reduce the dimension of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$512\\times 512\\times 1$$\\end{document}512\u00d7512\u00d71 into \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$224\\times 224\\times 1$$\\end{document}224\u00d7224\u00d71 for effective training of model.\n          To handle imbalance problem in data set because Brain tumor data set has the different number of three subjects slices. The distribution of the data is different, and it creates a problem of over fitting the model. To balance the meningioma, glioma, and pictutitary in the data set, we incorporate the data augmentation20 method to augment the original dataset by using random zooming. All slices are being zoomed, and a new data set with 6128 slices has been created. The ratio of samples in an original data set is shown in Fig. 3. The data set has three subfolders for meningioma, glioma, and pictutitary images. Held out techniques is used for model training and validation because the new data set is very big and heldout validation is suitable in case of plenty dataset. The data set has splitted into 70% and 30% for training and validation of the model respectively. The cross-validation method has also been employed for an augmented data set.\n          We also used the Brain MRI Images Data Set (BMIDS) for cross dataset validation, which contains 253 MRI brain images. The tumor class in the data set has 155 images, while the non-tumor class has 98 images.Figure 3Ratio of samples in data set.\nResults of the proposed CNN model, on original and augmented data sets\n          The performance of the proposed CNN model is evaluated using the original and augmented brain tumour MR image data sets. The CNN model is configured with essential hyper-parameters such as optimizer SGD with a Learning Rate (LR) of 00.0001, epochs 100, and size of batch was 120. The 70% data for training and 30% for the testing of the model is used. Different evaluation matrices were used for model performance evaluation. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 is used for training and evaluation of the proposed CNN model. All these hyper-parameters values and the output of the experimental results have been reported in Table 1.\n          Table 1 presented the proposed CNN model obtained 97.40% accuracy, 98.03% specificity, 95.10% sensitivity, 99.02% Precision, 97.75% MCC, and 97.26% F1-score respectively on original brain tumor MR images data set. The 97.40% accuracy demonstrated that our CNN architecture accurately classifies the three classes of brain tumors (meningioma, glioma, and pictutitary). The 98.03% specificity shows that the Proposed CNN model is a highly suitable detecting model for healthy subjects recognition, while 95.10% sensitivity presents that the model significantly detected the affected subjects. The MCC value was 97.75%, which gives confusion metrics a good summary.\n          On the other hand, the CNN model gained very excellent performance when trained and evaluated on an augmented data set. The CNN model obtained 98.56% accuracy, 100.00% specificity, 98.09% sensitivity, and 98.00% MCC when trained and evaluated on an augmented data set. The accuracy of the model improved from 97.40 to 98.56% which demonstrated the importance of the data augmentation process. Also, it illustrated that model needs more data for effective training of the CNN model.\n          From the experimental results, we concluded that the proposed CNN model effectively classified the brain tumor types, and the augmentation process further improved the model CNN performance because the CNN model more data for extract more related features for classification. The high accuracy of the proposed CNN model might be due to the suitable architecture of the CNN model and proper fitting of essential parameters of the model and data augmentation.Table 1CNN model performance on original and augmented data sets.Data setParametersAssessment metricsOptimizerLRAcc (%)Sp (%)Sn (%)Pr (%)MCC (%)F1-S (%)OriginalSGD0.000197.4098.0395.1099.0297.7597.26Augmented\u2013\u201398.56100.0098.0997.1298.0098.10\nCNN model performance evaluation with cross dataset\n          We have evaluated the predictive performance of CNN model with independent cross dataset. We trained the proposed CNN model with original and augmented brain tumor data set and validated with independent Brain MRI Images Data Set (BMIDS). The model is configured with essential hyper-parameters such as optimizer SGD with a Learning Rate (LR) of 00.0001, epochs 100, and size of batch was 120. Different evaluation matrices were used for model performance evaluation. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 is used for training and evaluation of the proposed CNN model. The experimental results of model with cross data are reported in Table 2.\n          Table 2 presented that the proposed CNN model obtained 97.96% accuracy, 99.00% specificity, 97.30% sensitivity, 98.18% Precision, 98.00% MCC, and 99.02% F1-score when trained on original brain tumor MR images data set (BTDS) and validated with independent data set (BMIDS).\n          Other other side the model achieved 98.97% accuracy, 99.89% specificity, 99.39% sensitivity, 98.89% Precision, 99.40% MCC, and 99.30% F1-score when trained with augmented data set (BTDS) and validated with independent data set (BMIDS). Hence, from experimental results we observed that model predictive and generalization capability improved when trained and validated with independent data sets.Table 2CNN model performance with cross data set.Data setParametersAssessment metricsOptimizerLRAcc (%)Sp (%)Sn (%)Pr (%)MCC (%)F1-S (%)OriginalSGD0.000197.9699.0097.3098.1898.0099.02Augmented\u2013\u201398.9799.8999.3998.8999.4099.30\nResults of the transfer learning models (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) on original and augmented data sets\n          The performances of transfer learning (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) models have checked on original and augmented data sets. Theses models have configured other essential hyper-parameters such as optimizer SGD with learning rate 0.0001, the number of epoch 100, batch size 120. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 is used for training and evaluation of the proposed model. The (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) models are evaluated using different performance evaluation metric. The (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) models hyper-parameters values and the output of the experimental results have reported in Table 3.Table 3Transfer learning models predictive performance on original and augmented data sets.ModelData setSpace complexityTime complexityLRAssessment metrics\u201325.6M3.2\u00a0h0.0001Acc (%)Sp (%)Sn (%)Pr (%)MCC (%)F1-S (%)ResNet50Original0.000197.0397.0493.1094.2193.2395.00Augmented\u201398.0799.30100.0096.0796.0097.00VGG-16\u2013138.4M3.02\u00a0h\u201394.7796.3094.6793.4391.9096.61\u2013\u201395.9796.9599.4096.8492.9896.80Inception V3\u201323.9M3.96\u00a0h\u201393.2396.8995.0096.0895.5697.87\u2013\u201396.0397.0397.0097.0196.0598.00DenseNet201\u201320.2M3.9\u00a0h\u201396.7695.9995.6095.7898.4598.23\u2013\u201397.4397.7899.3096.6198.4498.89Xception\u201322.9M4.3\u00a0h\u201393.0097.0398.0097.0999.3297.23\u2013\u201395.6098.9896.0098.0499.9898.00MobilleNet\u20134.3M2.6\u00a0h\u201396.7698.0999.5095.9896.6498.23\u2013\u201397.8799.0094.5696.2398.4597.89The space complexity of each model is the number of trainable parameters. M\u00a0= Million. The space complexity increases with increasing number of trainable parameters. The time complexity is the training time (in hours) of the models.\n          Table 3 show that the ResNet-50 model obtained 97.03% accuracy, 97.04% specificity, 93.10% sensitivity, 94.21% Precision, 93.23% MCC, and 95.00% F1-score respectively on original brain tumor data set. The 95.30% accuracy show that the ResNet-50 model accurately classifies the three classes of brain tumors (meningioma, glioma, and pictutitary). The 97.04% specificity shows that the ResNet-50 model is a highly suitable detecting model for healthy subjects recognition, while 93.10% sensitivity show that the model accurately detected the affected subjects.\n          The predictive Performance of transfer learning model ResNet-50 very high when model trained and evaluated with augmented data set. According to Table 3 the transfer learning model ResNet-50 obtained 98.07% accuracy, 99.30% specificity, 100.00% sensitivity, 96.07% precision, 96.00% MCC, and 97.00% F1-S, when trained and evaluated on augmented data set.\n          The VGG-16 model with original and augmented data sets obtained 94.77% accuracy, 96.30% specificity, 94.67% sensitivity, 93.43% precision, 91.90% MCC, 96.61% F1-S, and 95.97% accuracy, 96.95% specificity, 99.40% sensitivity, 96.84% precision, 92.98% MCC, and 96.80% F1-S respectively.\n          Inception V3 obtained 93.23% accuracy, 96.89% specificity, 95.00% sensitivity, 96.08% precision, 95.56% MCC, and 97.87% F1-s, with original data set. While on augmented data set Inception V3 obtained 96.03% accuracy, 97.03% specificity, 97.00% sensitivity, 97.01% precision, 96.05% MCC, 98.00% F1-S. DenseNet201 model obtained 96.76% accuracy on original data set and increase it 97.43% accuracy with augmented data set.\n          The Xception model with original data set achieved 93.00% accuracy, 97.03% specificity, 98.00% sensitivity, 97.09% precision, 99.32% MCC, 97.23% F1-S and obtained 95.60% accuracy, 98.98% specificity, 96.00% sensitivity, 98.04% precision, 99.98% MCC, and 98.00% F1-S with augmented data set. MobilleNet model obtained 96.76% accuracy with original data set and 97.87% with augmented data set. Among all models the ResNet-50 model performance in terms of accuracy is high with augmented data set. The model improved accuracy from 95.30 to 98.07% with data augmentation. The other evaluation metrics values also improved with data augmentation. From the experimental results, we concluded that the data augmentation process increased the training of ResNet-50 and model effectively classified the brain tumor types.\nResults of the integrated frameworks (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) on original and augmented data sets\n          The integrated frameworks (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) performances have checked on original and augmented data sets. Furthermore, we have incorporated the TL ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet CNN architectures with imageNet data set to generate high weights and then transferred trained parameters weights of these pre trained models to the CNN model individually for effective training of CNN model. For fine-tuning of the CNN model, the brain tumors original and augmented data sets have used for final classification. The models have configured with concern hyper-parameters such as optimizer SGD with learning rate 0.0001, the number of epoch 100, batch size 120. The proposed framework performance has been evaluated employing various matrices. The input image size \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$264\\times 264\\times 1$$\\end{document}264\u00d7264\u00d71 has been used for training and evaluation of the proposed model. All these hyper-parameters values and the output of the experimental results of (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) models have reported in Table 4.\n          Table 4 presented that the ResNet50-CNN model obtained 99.10% accuracy, 100.00% specificity, 89.60% sensitivity, 98.75% Precision, 98.66% MCC, and 99.5% F1-score respectively on original brain tumor data set. The 99.10% accuracy demonstrated that the architecture accurately classifies the three classes of brain tumors (meningioma, glioma, and pictutitary). The 100% specificity shows that the Proposed model is a highly suitable detecting model for healthy subjects recognition, while 89.60% sensitivity presents that the model significantly detected the affected subjects.\n          On the other hand, the model obtained very high performance when it trained and evaluated on the augmented data set. The integrated CNN and transfer learning model (ResNet-50-CNN) obtained 99.90% accuracy, 99.08% specificity, 96.13% sensitivity, and 99.10% MCC when trained and evaluated on augmented data set.\n          The VGG-16-CNN model with original and augmented data sets obtained 96.78% accuracy, 99.23% specificity, 95.00% sensitivity, 96.99% precision, 98.93% MCC, 97.98% F1-S, and 97.88% accuracy, 98.00% specificity, 100.00% sensitivity, 96.98% precision, 98.79% MCC, and 99.00% F1-S respectively.\n          Inception V3-CNN model obtained 97.00% accuracy, 99.00% specificity, 99.87% sensitivity, 98.92% precision, 95.76% MCC, 98.09% F1-S with original data set. While on augmented data set Inception V3 obtained 98.02% accuracy, 100.00% specificity, 98.67% sensitivity, 97.56% precision, 99.00% MCC, and 97.30% F1-S.\n          DenseNet201-CNN model obtained 97.00% accuracy on original data set and increase it 97.90% accuracy with augmented data set. Hence, the integrated model DenseNet201-CNN improved accuracy 97.00\u201397.90%\u00a0= 0.90% with data augmentation process.\n          The Xception-CNN model with original data set achieved 98.20% accuracy, 98.88% specificity, 97.40% sensitivity, 99.00% precision, 99.10% MCC, 98.65% F1-S, and obtained 98.97% accuracy, 99.00% specificity, 98.60% sensitivity, 97.24% precision, 97.99% MCC, 99.30% F1-S with augmented data set. MobilleNet-CNN model obtained 98.08% accuracy with original data set and 98.56% with augmented data set. The improved accuracy 98.08% to 98.56% when model fine tuned with augmented data set.\n          From above anlaysis we conculded that among all the ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN, the predictive performance of ResNet-50-CNN model is high in terms of accuracy. The accuracy of the model improved from 99.10 to 99.90% which is illustrated the importance of the data augmentation and transfer learning process. Hence we concluded that the ResNet-50-CNN model effectively classify the brain tumor types. The high accuracy of the proposed integrated diagnosis framework might be due to the suitable architecture of the model and proper fitting of essential parameters of the model and data augmentation. In addition, the proposed integrated model (ResNet-50-CNN) accuracy has compared with CNN model and transfer learning ResNet-50 model in Table 5 on augmented data set and graphically shown in Fig. 4.Table 4Integrated frameworks (ResNet-50-CNN, VGG-16-CNN, Inception V3-CNN, DenseNet201-CNN, Xception-CNN, and MobilleNet-CNN) performance on original and augmented data sets.ModelData setSpace complexityTime complexityLRAssessment metrics\u201325.6M3.3h0.0001Acc(%)Sp(%)Sn(%)Pr(%)MCC(%)F1-S(%)ResNet50-CNNOriginal0.000199.10100.0089.6098.7598.6699.5Augmented\u201399.9099.0896.1399.2299.1099.43VGG-16-CNN\u2013138.4M3.21\u00a0h\u201396.7899.2395.0096.9998.9397.98\u2013\u201397.8898.00100.0096.9898.7999.00Inception V3-CNN\u201323.9M4.13\u00a0h\u201397.0099.0099.8798.9295.7698.09\u2013\u201398.02100.0098.6797.5699.0097.30DenseNet201-CNB\u201320.2M4.06\u00a0h\u201397.0099.9998.1199.1297.9899.09\u2013\u201397.90100.0097.4595.6899.0399.32Xception-CNN\u201322.9M4.55\u00a0h\u201398.2098.8897.4099.0099.1098.65\u2013\u201398.9799.0098.6097.2497.9999.30MobilleNet-CNN\u20134.3M2.7\u00a0h\u201398.0899.4393.2099.1297.6593.231\u2013\u201398.56100.0099.7693.9899.3299.05The space complexity of each model is the number of trainable parameters. M\u00a0= Million. The space complexity increases with increasing number of trainable parameters. The time complexity is the training time (in hours) of the models.Table 5Accuracy of CNN, ResNet-50 and ResNet-50-CNN on augmented data.CNN (Acc%)ResNet-50 (Acc%)ResNet-50-CNN (Acc%)98.9798.0799.90\n          \nFigure 4CNN, ResNet-50 and Integrated (ResNet-50-CNN) models accuracy comparison with augmented Brain tumor data set. The CNN model obtained accuracy\u2019s with augmented data is 98.97%, while ResNet-50 obtained 96.07% and Integrated model ResNet-CNN obtained high predictive accuracy 99.90% with augmented data. Thus, the proposed integrated ResNet-CNN model is suitable for effective classification of brain tumors and could assist clinical professionals to diagnosis brain cancer accurately and efficiently. Due to the high performance of proposed ResNet-CNN method we recommend it for diagnosis of brain cancer in IoT-healthcare.\nAccuracy comparison of the proposed (ResNet-CNN) model with state of-the-art models\n          We have compared our ResNet-50-CNN (ResNet-CNN) model performance in terms of accuracy with state-of-the-art methods in Table 6. Table 6 and Fig. 5 presented that proposed model obtained 99.89% accuracy, which is high as compared to state-of-the-art techniques. The high performance of the proposed method demonstrated that it is correctly classified brain tumors (meningioma, glioma, and pictutitary), and it can easily be deployed in IoT-health care for the classification of brain tumors.Table 6Comparison of ResNet-CNN model accuracy with previous models.ModelAcc (%)RefSpace complexityTime cmplexityANN and KNN97, 9810\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((nm + mk) + (n*d) )$$\\end{document}O((nm+mk)+(n\u2217d))\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((cwh) + (nd+Kn))$$\\end{document}O((cwh)+(nd+Kn))GA-CNN94.212\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)SVM and KNN85, 888\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((n) + (n*d))$$\\end{document}O((n)+(n\u2217d))\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}((n^2) + (nd+Kn))$$\\end{document}O((n2)+(nd+Kn))CNN-TF94.8213\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)Proposed method ResNet-CNN99.902022\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m)\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c=$$\\end{document}c= the number of convolutional channels, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h=$$\\end{document}h=\u00a0height of input, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w=$$\\end{document}w= width of input, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f=$$\\end{document}f= the convolutional kernel size, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n=$$\\end{document}n= the number data instances, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k=$$\\end{document}k= the number of output neurons, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m=$$\\end{document}m= the number of input neurons and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d =$$\\end{document}d= the dimension or feature of the input, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$K=$$\\end{document}K=\u00a0number of nearest neighbors, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u=c*w*h$$\\end{document}u=c\u2217w\u2217h.Figure 5ResNet-CNN model performance comparison with baseline models show that our model predictive performance in terms of accuracy is high from baseline models. The ResNet-CNN model cloud accurately and efficiently classify the brain tumors and assist medical experts to interpret the images of brain tumors to diagnosis brain cancer.\nSpace and time complexity\n          Also, in Tables 3, 4, and 6, we present both the models space and complexity of the various proposed methods used in the prediction of Brain cancer. Since the proposed models are convolutional deep learning methods, the space complexities are analyzed in terms of the each model\u2019s trainable parameters. For the time complexity, the model\u2019s training time is used. It could be deduced from Table 3 that VGG-16 has the worst space complexity since its trainable parameter is 138.4 million, whiles MobileNet has the best space time complexity. Moreover for the time complexity, the Xception model has the worst time complexity because its training time is 4.3\u00a0h. Because of the difficulty of accessing the models of the competing methods in Table 4 , we could not experimentally analyze the complexity of the models in terms of algorithmic run-time. It is more likely that almost all the methods with the deep learning techniques, the convolutional neural networks will have a worse space and time complexity because of the significant number of parameters and matrix computation that come with the models\u2019 architecture. Irrespective of the worst case time and space complexity, our proposed model has an accuracy performance gain as compared to all competing methods. The time complexity is the training time (in hours) of the models as reported in Tables 3, 4, and 6. The space and time complexity of our model are \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(cwh + 1)f$$\\end{document}O(cwh+1)f and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {O}(f*u*m)$$\\end{document}O(f\u2217u\u2217m) respectively.\nDiscussion\n      Brain Tumor Classification using MR images are critical in the detection of brain cancer In IoT healthcare systems. Artificial intelligence (AI) based computer automatic diagnostic systems (CAD) can effectively different diagnose diseases in IoT healthcare system. Deep learning techniques are widely used in CAD systems to diagnose critical diseases32, especially convolutional neural networks. The CNN model is mostly used for medical image classification18,19. The CNN model extracts deep features from image data, and these features played an important role in final image classification. For the diagnosis of brain cancer, various methods have been proposed by researchers using brain MR image data and deep learning models. However, these existing methods have lack of accuracy of diagnosis. In order to tackle this problem, a new method is necessary to diagnose the disease accurately and efficiently IoT healthcare systems.\n      In this study, we have proposed a CNN model for the accurate classification of brain tumor using Brain MR images. In the design of the proposed method, we have applied the deep learning CNN model for the classification of tumors meningioma, gLioma, and pituitary. The CNN model extracts more deep features from image data for final classification. To further improve the CNN model predictive capability, we have incorporated a transfer learning mechanism because, for proper training of the CNN architecture, the brain MR images data is insufficient. In transfer learning, we have used the well-known pre-trained models (ResNet-50, VGG-16, Inception V3, DenseNet201, Xception, and MobilleNet) with big imageNet data set to generate high parameters (weights). These generated weights of models individually transferred to CNN model for effective training. For the fine-tuning process, the model was trained with brain MR images data set. Also, the data augmentation method is employed to increase the data set size for effective training of the model. Furthermore, we have used held-out cross-validation and performance evaluation metrics. We also used cross data set for cehcking the propoed CNN model predictice performance.\n      According to Tables 2, 3, 4 and 6 the proposed method obtained high results as compared to baseline methods. The high performance of the proposed ResNet-CNN model might be due to the proper setting of model parameters such as learning rate, batch size, number of the epoch, and pre-processing, and data augmentation. We recommend the proposed method for meningioma, gLioma, and pituitary classification. Furthermore, the proposed method would be applied for diagnosis of a brain cancer in IoT-Healthcare systems easily.\nConclusion\n      For accurate medical image classification, the CNN model is played a significant role, and in most CAD systems CNN model is used for the analysis of medical image data. In research study, we have proposed a deep learning-based diagnosis approach for brain tumor classification. In the proposed method, we have used a deep CNN model for the classification of tumor types Meningioma, Glioma, and Pituitary employing brain tumor MR images data. To enhance the predictive capability of the CNN model, we have incorporated transfer learning and data augmentation techniques. The experimental results show that the proposed integrated diagnosis framework ResNet-CNN has obtained 99.90% accuracy as compared to baseline methods. The high predictive outcomes of the proposed method might be due to the effective pre-processing of data and the adjustment of other parameters of the model such as numbers of layers, optimizer and activation functions, transfer learning, and data augmentation. Due to the high performance of the proposed ResNet-CNN model, it could be applicable for the classification of brain tumors and diagnosis of brain cancer in IoT-Healthcare. In the future, we will use other brain tumors datasets and other deep learning techniques to diagnose brain tumors.\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC9468046/\nhttps://doi.org/10.1038/s41598-022-19465-1",
    "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9468046/",
    "doi": "https://doi.org/10.1038/s41598-022-19465-1",
    "citation_count": 10,
    "references": {
        "10978992": "Revolutionizing heart disease prediction with quantum-enhanced machine learning",
        "10944839": "DDFC: deep learning approach for deep feature extraction and classification of brain tumors using magnetic resonance imaging in E-healthcare system",
        "10486696": "Brain\u2013Computer Interface: The HOL\u2013SSA Decomposition and Two-Phase Classification on the HGD EEG Data",
        "10453237": "Bridged-U-Net-ASPP-EVO and Deep Learning Optimization for Brain Tumor Segmentation",
        "10422344": "An Augmented Modulated Deep Learning Based Intelligent Predictive Model for Brain Tumor Detection Using GAN Ensemble",
        "10255698": "Deep Learning-Based IoT System for Remote Monitoring and Early Detection of Health Issues in Real-Time",
        "10178263": "U-Net-Based Models towards Optimal MR Brain Image Segmentation",
        "10067827": "An integrative machine learning framework for classifying SEER breast cancer",
        "10046932": "Grade Classification of Tumors from Brain Magnetic Resonance Images Using a Deep Learning Technique",
        "10001035": "Refined Automatic Brain Tumor Classification Using Hybrid Convolutional Neural Networks for MRI Scans"
    },
    "journal": "Scientific Reports",
    "topics": [
        "iot, approach, environment, tumors",
        "learning, dacbt, mri, classification",
        "environment, deep, brain, iot"
    ]
}